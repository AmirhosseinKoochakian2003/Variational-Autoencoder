{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b8be54-9eea-4900-8d39-4d9545cbaef2",
   "metadata": {},
   "source": [
    "# Variational autoencoders\n",
    "\n",
    "In this project, we will look into the concept of variational autoencoders. We will also create a variational autoencoder using TensorFlow to generate images similar to the MNIST handwritten digits.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Introduction](#1)\n",
    "- [2 - Autoencoders](#2)\n",
    "- [3 - Variational autoencoders](#3)\n",
    "    - [3.1 - Theory behind VAEs](#3.1)\n",
    "        - [3.1.1 - Evidence Lower BOund](#3.1.1)\n",
    "        - [3.1.2 - Variational Inference](#3.1.2)\n",
    "        - [3.1.3 - Reparameterization Trick](#3.1.3)\n",
    "        - [3.1.4 - The probability model perspective](#3.1.4)\n",
    "        - [3.1.5 - The autoencoder perspective](#3.1.5)\n",
    "- [4 - Implementation](#4)\n",
    "    - [4.1 - Dimension Reduction using Autoencoders](#4.1)\n",
    "    - [4.2 - Hand-written Image Generator](#4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47500df-22a0-46de-9ff1-5378782bcb7d",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Introduction\n",
    "Variational autoencoders (VAEs) are a type of deep generative models that encode input data into a lower-dimensional space, randomly sample points from this latent space, and then decode these points to generate new data. More precisely, VAEs can be viewed in two different ways:\n",
    "\n",
    "1. VAEs are a type of autoencoder with a key distinction from traditional autoencoders (AEs), which will be explained later when we discuss variational autoencoders.\n",
    "2. VAEs are probabilistic generative models that operate on the joint distribution of observed and latent variables.\n",
    "\n",
    "In this notebook, we will provide a brief overview of autoencoders, variational autoencoders, delve into the underlying mathematics, and demonstrate the implementation of both an autoencoder and a variational autoencoder with the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ffa3d-ba6c-4eff-b2da-d8245aaa129a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## Autoencoders\n",
    "An autoencoder is a type of neural network used to reduce high-dimensional data to a lower-dimensional representation. It consists of three parts: encoder, bottleneck, and decoder. The encoder compresses the data to a lower-dimensional form, the bottleneck stores the crucial information, and the decoder reconstructs the output from the bottleneck. During training, the output is compared to the original input to calculate reconstruction loss, which is the squared error between the input and output. Essentially, training an autoencoder involves optimizing the encoder and decoder to minimize the reconstruction error.\n",
    "\n",
    "Applications of autoencoders include:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Image denoising\n",
    "- Anomaly detection\n",
    "\n",
    "| <img src=\"https://miro.medium.com/v2/resize:fit:600/0*83pAHsVsnsqySOHN.png\" width=\"400\" height=\"300\"> | \n",
    "|:--:| \n",
    "| *Autoencoder architecture* |\n",
    "*source : [Medium](https://medium.com/@soumallya160/the-basic-concept-of-autoencoder-the-self-supervised-deep-learning-454e75d93a04)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca474c4-4ae8-424f-8ce5-b397602af844",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## Variational autoencoders\n",
    "\n",
    "A variational autoencoder (VAE) is a type of probabilistic autoencoder designed to model the distribution of data. They share a similar architecture with standard autoencoders, featuring an encoder, bottleneck, and decoder. However, the way information is processed in the bottleneck of a VAE differs from that of a traditional autoencoder.\n",
    "\n",
    "VAEs can be understood in two main ways:\n",
    "\n",
    "1. Probabilistic Generative Model: VAEs function as probabilistic generative models, employing a Bayesian approach to learn the probability distribution of input data.\n",
    "2. Autoencoder: While VAEs belong to the family of autoencoders, they differ in that they are probabilistic rather than deterministic. In a VAE, the compressed representation (latent vector ùëß) is not directly learned but is generated randomly based on the learned distribution.\n",
    "\n",
    "\n",
    "| <img src=\"data:image/png;base64,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\" width=\"400\" height=\"300\"> | \n",
    "|:--:| \n",
    "| *Variational autoencoder architecture* |\n",
    "*source : [Medium](https://www.google.com/imgres?imgurl=https%3A%2F%2Fmiro.medium.com%2Fv2%2Fresize%3Afit%3A1358%2F0*2DZwupQZTnpBEH1s.png&tbnid=6LvLReF4DrHNNM&vet=12ahUKEwig6JCg5rKEAxX4BfsDHW-xCewQMygSegQIARB0..i&imgrefurl=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fvariational-auto-encoders-vae-for-the-numerai-dataset-2709dcc7e449&docid=Zkc9F34LppKCyM&w=1264&h=524&q=variational%20auto%20encoder&ved=2ahUKEwig6JCg5rKEAxX4BfsDHW-xCewQMygSegQIARB0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014ece3-03c2-4c6f-bbf2-1f4c19f769d3",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### Theory behind VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a4cc-ee4b-4170-aa13-81907d806299",
   "metadata": {},
   "source": [
    "<a name='3.1.1'></a>\n",
    "#### Evidence Lower BOund\n",
    "The Evidence Lower Bound (ELBO) plays a crucial role in approximating the posterior distribution. It serves as a minimum value for the logarithm of the total likelihood of the data given the model parameters. By maximizing the ELBO, we effectively minimize the Kullback-Leibler (KL) divergence between the actual posterior distribution and the approximated one.\n",
    "\n",
    "Before delving into the lower bound concept, let's lay out the problem. Let's assume we have two random variables: $X$ (observed or real data) and $Z$ (hidden or latent variable). Both $X$ and $Z$ follow a joint distribution denoted as $p(x,z;\\theta)$, where $\\theta$ represents the distribution's parameters.\n",
    "\n",
    "Now, what exactly is the evidence? The evidence simply refers to the log-likelihood of observations ùë• under a fixed $\\theta$. Essentially, the likelihood function indicates how well our model and parameter $\\theta$ match the observations. A high likelihood value suggests that the model fits the data well. Our objective now is to find a lower bound for $p(x;\\theta)$. Let's assume that $z$ follows a distribution denoted as ùëû. We can leverage a technique called marginalization to derive the desired lower bound.\n",
    "\n",
    "$$\\begin{align} log p(x;\\theta) &= log\\int_{z} p(x,z;\\theta) dz \\\\ &= log\\int_{z} p(x,z;\\theta) \\frac{q(z)}{q(z)} dz \\\\ &= log E_{z\\sim q}[\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ & \\geq^* E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}]\\end{align}$$\n",
    "\n",
    "$$\\begin{align} => ELBO = E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}] \\ (1) \\end{align}$$\n",
    "\n",
    "\\* : This inequality is the result of [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality). Since $log$ is a concave function, $log(E[X]) \\geq E[log(X)]$ where X in this statement is $\\frac{p(x,Z;\\theta)}{q(Z)}$.\n",
    "\n",
    "Now, let's explain why maximizing the Evidence Lower Bound (ELBO) leads to minimizing the Kullback-Leibler (KL) divergence between the true posterior distribution and the approximating distribution. Let's assume we aim to identify a distribution, denoted as $q$, that accurately approximates $p(z|x; \\theta)$. In the context of Variational Autoencoders (VAEs), our goal is to approximate $p(z|x; \\theta)$. This requires us to find a distribution that closely approximates it, which is why I am discussing this specific distribution.\n",
    "\n",
    "$$\\begin{align} KL \\ (q(z) \\ || \\ p(z|x;\\theta)) &= E_{Z \\sim q}[log \\frac{q(Z)}{p(Z|x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z|x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\frac {p(Z,x;\\theta)}{p(x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z,x;\\theta)] + E_{Z \\sim q}[log \\ p(x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ p(x;\\theta)] - E_{Z \\sim q}[log\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ &=^{*,(1)} log \\ p(x;\\theta) - ELBO \\\\ &= evidence - ELBO\\end{align}$$\n",
    "\n",
    "\\* : Notice that  $log p(x;\\theta)$ is not dependent on  $z$, so it behaves like a constant and can be taken out of the expectation.\n",
    "\n",
    "Now, consider that when $\\theta$ is fixed and we are searching for a distribution q that minimizes the Kullback-Leibler (KL) divergence, the evidence term acts as a constant. Therefore, due to the negative sign of the Evidence Lower Bound (ELBO), maximizing the ELBO results in minimizing the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff2f0a-dde6-44d0-a97b-16a728f15c53",
   "metadata": {},
   "source": [
    "<a name='3.1.2'></a>\n",
    "#### Variational Inference\n",
    "\n",
    "In many real-world situations, it's impractical to calculate the exact probability distribution due to either complex models or large datasets. Variational inference offers a method to estimate this distribution by selecting a simpler distribution from a set of parameterized distributions. The concept of variational inference involves treating the task of approximating the probability distribution as an optimization challenge. The objective is to identify the distribution within the chosen set that minimizes the difference, measured by Kullback-Leibler divergence, from the true distribution.\n",
    "\n",
    "By maximizing the Evidence Lower Bound (ELBO), a surrogate for the log likelihood of the data based on the model parameters, variational inference aims to find the most precise approximation to the actual probability distribution given the model and observed data.\n",
    "\n",
    "When dealing with models containing hidden ($Z$) and observed ($X$) variables and trying to determine the likelihood of $Z$ given $X$, variational inference aids in finding a suitable estimation. However, let's first understand why we cannot directly calculate  $p(z|x)$ . By applying Bayes' theorem,  $p(z|x)$  can be expressed as follows:\n",
    "\n",
    "$$\\begin{align} p(z|x) = \\frac{p(z,x)}{p(x)} = \\frac{p(x|z) \\times p(z)}{\\int_z p(z,x) dz} \\end{align}$$\n",
    "\n",
    "The key thing to note is that computing the denominator is not always possible. Therefore, variational inference is utilized.\n",
    "\n",
    "In this method, we select a group of distributions called the variational distribution family and strive to discover a $q$ that comes closest to  $p(z|x)$ within this family. Let's also assume that $\\phi$ controls these distributions (it's the variational parameter). Hence, our aim is to identify a $\\phi$ that minimizes the KL divergence between  $q_{\\phi}(z)$ and  $p(z|x)$ . Interestingly, as indicated by the [Evidence Lower BOund](#3.1.1), instead of minimizing the KL divergence, we can maximize the ELBO to find the most accurate approximation. Therefore, we must now handle $\\phi$ as it is a parameter that characterizes the family of distributions.\n",
    "\n",
    "$$\\begin{align} ELBO(\\phi) &= L(\\phi) = E_{Z \\sim q_{\\phi}}[log \\ p(x,Z) - log \\ q(Z)] \\\\ \\hat{\\phi} &= argmax_{\\phi} \\ L(\\phi) \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5996d0c-b686-480d-9cbf-f5d271642e53",
   "metadata": {},
   "source": [
    "<a name='3.1.3'></a>\n",
    "#### Reparameterization Trick\n",
    "\n",
    "In a typical variational inference scenario, our goal is to estimate the distribution of hidden variables using a variational distribution. When the variational distribution involves sampling from a distribution like a Gaussian, it introduces randomness into the calculation of gradients during the optimization procedure. The reparameterization trick helps to tackle this problem by modifying the way the random variable is represented so that the randomness is isolated from the distribution's parameters. This allows gradient-based optimization methods to navigate through the random sampling process, resulting in more consistent training with lower variance gradient estimations. Therefore, this approach enables us to conduct stochastic gradient ascent on the ELBO. Now that we understand how we approximate the hidden variable distribution, we will employ optimization techniques to determine the optimal parameters in our scenario.\n",
    "\n",
    "##### **High Variance Gradients**\n",
    "First, let's consider what happens if we don't employ the reparameterization trick. For instance, suppose we aim to compute the gradient with respect to the parameter $\\theta$ of the expectation $E_{x \\sim p}[f_{\\theta}(x)]$. If the distribution $p$ is not determined by $\\theta$, then the gradient can straightforwardly be incorporated within the expectation. Nevertheless, if this is not the scenario, then in general, we are unable to compute the gradient.\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\theta} \\ E_{x \\sim p_{\\theta}}[f_{\\theta}(x)] &= \\nabla_{\\theta} \\int_{x} p_{\\theta}(x) f_{\\theta}(x) dx \\\\ &= \\int_{x}\\nabla_{\\theta} ( p_{\\theta}(x) f_{\\theta}(x)) dx \\\\ &= \\int_{x}\\nabla_{\\theta}p_{\\theta}(x) f_{\\theta}(x)dx + \\int_{x} p_{\\theta}(x) \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &= \\int_{x}\\nabla_{\\theta}p_{\\theta}(x) f_{\\theta}(x)dx + E_{x \\sim p_{\\theta}}[\\nabla_{\\theta}f_{\\theta}(x)]\\end{align} $$\n",
    "\n",
    "The first term in the final expression may not always be an expectation. Monte Carlo methods need the ability to randomly select values from $p(x)$ but don't necessarily need its gradient information. It gets tricky when we can't easily calculate the gradient of $p(x)$ with respect to $\\theta$, which is often the case in real-world situations. To make Monte Carlo methods work, we have to make adjustments to eliminate $\\nabla_{\\theta} p_{\\theta}(x)$ from our calculations.\n",
    "\n",
    "##### **Reparameterization Trick Explanation**\n",
    "At times, the variable $x$ sampled from $p_{\\theta}(x)$ can be represented differently as a fixed function $g$ of $\\theta$ and another random variable $\\epsilon$ sampled from $D$, where $D$ is independent of $\\theta$.\n",
    "$$\\begin{align} \\epsilon &\\sim D \\\\ x &= g_{\\theta}(\\epsilon) \\end{align}$$\n",
    "\n",
    "Now lets see what happen if we take gradient from $E_{x \\sim p}[f_{\\theta}(x)]$:\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\theta} \\ E_{x \\sim p_{\\theta}}[f_{\\theta}(x)] &= \\nabla_{\\theta} \\ E_{\\epsilon \\sim g_{\\theta}}[f(g_{\\theta}(\\epsilon))] \\\\ &=^* E_{\\epsilon \\sim g_{\\theta}}[\\nabla_{\\theta} f(g_{\\theta}(\\epsilon))]\\end{align} $$\n",
    "\n",
    "\\* : Notice that $\\epsilon$ or more generally $D$ are not dependent on $\\theta$, so we can move the gradient inside the expectation.\n",
    "\n",
    "As we showed, now we can use the Monte Carlo method to estimate this expectation.\n",
    "\n",
    "One may wonder how the $D$ distribution can be determined. Usually, it is not an easy problem, but certain families of distributions lead to straightforward solutions. For instance, in our scenario, where we deal with Gaussian distributions ($q(z|x)$), we can opt for a standard normal distribution and our function $g_{\\theta}(\\epsilon)$ as: $g_{\\theta}(\\epsilon) = \\mu + \\sigma\\epsilon$.\n",
    "\n",
    "To employ the reparameterization trick, we must also consider these requirements:\n",
    "\n",
    "- $f_{\\theta}(x)$ must be differentiable with respect to its input $x$.\n",
    "- $g_{\\theta}(\\epsilon)$ must exist and be differentiable with respect to $\\theta$.\n",
    "\n",
    "| <img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60e424b06f61a263edba1fe6_diagrammetic.png\" width=\"400\" height=\"200\"> | \n",
    "|:--:| \n",
    "| *Reparameterization trick* |\n",
    "*source : [V7 Labs](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.v7labs.com%2Fblog%2Fautoencoders-guide&psig=AOvVaw0-51ffMJPkS80TKsYK3FhU&ust=1708322395990000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCLDu-YSbtIQDFQAAAAAdAAAAABAP)*\n",
    "\n",
    "##### **Stochastic Gradient Ascent of the ELBO**\n",
    "For this aspect, we will apply the following concept to our initial issue, as discussed in [Evidence Lower Bound](#3.1.1) and [Variational Inference](#3.1.2), so all notations and symbols are in relation to these sections.\n",
    "\n",
    "The aim of utilizing variational inference is to discover the most similar $q$ to $p(z|x)$ by employing continuous optimization methods such as gradient ascent. As a result, parameters are updated at each iteration in the following manner:\n",
    "\n",
    "$$\\begin{align} \\phi_{t+1} = \\phi_t + \\alpha \\ \\nabla_{\\phi} \\ L(\\phi) \\end{align}$$\n",
    "Notice that we are attempting to maximize the ELBO, so we use gradient ascent.\n",
    "\n",
    "As we discussed earlier, ELBO involves an average, so the gradients encounter randomness during backpropagation. To eliminate this randomness, we can employ the reparameterization trick. Before delving into this concept, we need to address another issue: how can we calculate gradients? The complexity of our computations arises from dealing with integrals. Hence, we utilize stochastic gradient descent, which assumes a distribution for gradients. At each iteration, a gradient is sampled from that distribution to update the $\\phi$. Instead of computing the exact gradient of the ELBO concerning $\\phi$, we define a random variable $V(\\phi)$ such that $E[V(\\phi)] = \\nabla_{\\phi}L(\\phi)$ (this is simply a formal way to represent SGD).\n",
    "$$\\begin{align} v &\\sim V(\\phi) \\\\ \\phi_{t+1} &= \\phi_{t} + \\alpha \\ v \\end{align}$$\n",
    "\n",
    "First, we will reparameterize $q_{\\phi}(z)$ in the following way:\n",
    "$$\\begin{align} \\epsilon &\\sim N(0,1) \\\\ z &\\sim g_{\\phi}(\\epsilon) \\end{align}$$\n",
    "\n",
    "Where $g$ has the form that we talked in previous section.\n",
    "\n",
    "Now we replace every $z$ in ELBO equation with $g_{\\phi}$:\n",
    "$$\\begin{align} L(\\phi) = E_{\\epsilon \\sim N(0,1)}[log \\ p(x,g_{\\phi}(\\epsilon)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon))]\\end{align}$$\n",
    "\n",
    "As we demonstrated earlier, this reparameterization allows us to estimate the ELBO using Monte Carlo sampling. We will draw L samples from a standard normal distribution, where the ith sample is denoted as $\\epsilon_i$, and then estimate the expectation as:\n",
    "\n",
    "$$\\hat{L(\\phi)} = \\frac{1}{L} \\sum_{l=0}^{L}[log \\ p(x,g_{\\phi}(\\epsilon_l)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon_l))]$$\n",
    "\n",
    "Since all the requirements are satisfied, we can take the gradient on both sides:\n",
    "$$\\nabla_{\\phi} \\ \\hat{L(\\phi)} = \\nabla_{\\phi} \\frac{1}{L} \\sum_{l=0}^{L}[log \\ p(x,g_{\\phi}(\\epsilon_l)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon_l))]$$\n",
    "It is easy to demonstrate that taking the gradient of the loss function ($\\nabla_{\\phi} L(\\phi)$) and calculating its expectation ($E[\\nabla_{\\phi} L(\\phi)]$) are equivalent (simply need to move the $\\nabla_{\\phi}$ outside the expectation and bring the expectation inside the sigma). Therefore, $E[\\nabla_{\\phi} L(\\phi)]$ is the desired outcome.\n",
    "$$V(\\phi)= \\nabla_{\\phi} \\ \\hat{L(\\phi)}$$\n",
    "In simpler terms, when we sample values $\\epsilon_1,\\dots,\\epsilon_L$ from a standard normal distribution ($N(0,1)$), estimate an approximate ELBO, and calculate the gradient for this estimation, it's like randomly selecting gradients from a distribution $V(\\phi)$ where the average gradient aligns with the ELBO gradient. This underscores the significance of having $D$ easy to sample from when employing the reparameterization trick: we use samples from $D$ to generate samples for $V(\\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdb1e5-05bf-4098-9859-9c4361c3f162",
   "metadata": {},
   "source": [
    "<a name='3.1.4'></a>\n",
    "#### The probability model perspective\n",
    "Now that we have all the necessary tools to explain the complete theory behind VAEs, let's dive into the probabilistic aspect of VAEs.\n",
    "\n",
    "Variational autoencoders (VAEs) operate as generative models by establishing a joint distribution over samples and their corresponding hidden variable, denoted as $p(x,z)$. To explore this further, let's define the essential components:\n",
    "\n",
    "- Latent Variable (Z): In VAEs, a hidden variable $z$ plays a vital role in representing intricate data. The model depends on a simplified conditional distribution $p(x,z)$, where $x$ represents observable data, and $z$ remains hidden. This distribution is represented as $p(x,z) = p(x|z)p(z)$, indicating the probability of observed data given the hidden variable and the distribution of the hidden variable itself. This methodology empowers VAEs to capture complex patterns and generate meaningful data representations.\n",
    "- Observation Variable (X): The variable $x$ in VAEs signifies the observed data, and its distribution is characterized by $p(x|z)$, representing the probability of observing $x$ given a latent variable $z$. This probability distribution guides the VAE framework in learning and generating significant representations of input data, with $z$ capturing concealed patterns and facilitating varied sample generation during training.\n",
    "\n",
    "The generative process unfolds as follows:\n",
    "\n",
    "1. Sample a latent variable $z$ from a prior distribution, usually $N(0,I)$.\n",
    "2. Map $z$ to the parameters of another distribution to sample $x$ using $p(x|z)$.\n",
    "\n",
    "\n",
    "In Variational Autoencoders (VAEs), both distributions are considered normal distributions. Currently, we can calculate $p(x,z)$ using $p(z)$ and $p(x|z)$ by assuming we have suitable parameters, $\\theta$, for them. However, since $z$ is a hidden variable, we lack any information about it in our dataset. Consequently, the primary goal in VAEs is to estimate the posterior distribution, $p(z|x)$, instead of the prior distribution $p(z)$ or the likelihood $p(x|z)$. The hidden nature of the variable $z$ in the dataset sparks an interest in deducing the posterior distribution for each data point, showcasing the model's response to the observed data.\n",
    "\n",
    "By focusing on $p(z|x)$, VAEs mold the latent space to the specific characteristics of the observed data, integrating information, managing variability, and optimizing for effective reconstruction. This data-driven approach aligns with the generative process, enhancing representation learning and ensuring that the latent space captures meaningful patterns for improved generative capabilities. While seeking $p(z|x)$, we can derive $p(x)$. Thus, our objectives in VAEs are(Assuming we have a dataset containing $x_1,\\dots,x_n$, we can simplify the scenario by assuming that they are independently and identically distributed (i.i.d.).):\n",
    "\n",
    "1. For a fixed $\\theta$, compute $p_{\\theta}(z_i|x_i)$ for each $x_i$.\n",
    "2. Maximize the likelihood estimation (MLE) of $\\theta$.\n",
    "\n",
    "Nevertheless, both tasks cannot be computed directly due to the need for approximation. To address this, variational inference is employed. Initially, assuming $\\theta$ is fixed, we turn to variational inference and evidence lower bound to approximate $p_{\\theta}(z_i|x_i)$. Variational Inference selects a family of distributions and determines the closest distribution to $p_{\\theta}(z_i|x_i)$.\n",
    "\n",
    "Before defining our variational family, let's denote the function that calculates the mean and standard deviation of $x$ as $h_{\\phi}$ ($\\phi$ is the parameter of the $h$ function). It's essential to note that in variational autoencoders, this $h$ function is a neural network.\n",
    "\n",
    "$$\\begin{align} \\mu &= h_{\\phi}^{(1)}(x) \\\\ \\sigma^2 &= h_{\\phi}^{(2)}(x) \\end{align}$$\n",
    "\n",
    "$$variational \\ family = \\{q_{\\phi} | q_{\\phi} \\ \\sim N(h_{\\phi}^{(1)}(x), diag(h_{\\phi}^{(2)}(x)))\\}$$\n",
    "\n",
    "\n",
    "To create a loss function that measures the similarity between these two distributions, we utilize the Kullback‚ÄìLeibler divergence between the $q_{\\phi}(z_i|x_i)$ from a variational family (approximation distribution) and $p_{\\theta}(z_i|x_i)$. It's important to note that reducing the KL divergence is the same as enhancing the Evidence Lower Bound (ELBO).\n",
    "\n",
    "$$\\begin{align} L(\\phi) &= E_{Z_1,\\dots,Z_n \\sim q_{\\phi}}\\sum_{i=1}^n[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] \\\\ &= \\sum_{i=1}^nE_{Z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)]\\end{align}$$\n",
    "\n",
    "So far, we have assumed that $\\theta$ is fixed, and we have not optimized the ELBO with respect to $\\theta$. However, $\\theta$ represents the parameter of the decoder function, and we need to optimize both the decoder and the encoder. Therefore, we can expand the definition of our loss function and view the ELBO as a function of both $\\phi$ and $\\theta$. Our goal now is to find the optimal values for both $\\phi$ and $\\theta$ that maximize the ELBO.\n",
    "\n",
    "To explain why this is true: The equation $L(\\phi,\\theta) = evidence(\\theta) - KL(q_{\\phi}(z)||p_{\\theta}(z|x))$ shows that the ELBO is a lower bound for the log likelihood. By maximizing the ELBO with respect to both $\\phi$ and $\\theta$, the evidence will increase by at least the amount of the ELBO. This implies that the KL divergence is either decreasing or staying the same. Another way to justify this is by separately considering the gradient ascent process for $\\phi$ and $\\theta$.\n",
    "$$\\begin{align} L(\\theta, \\phi) &=  log \\ p_{\\theta}(x) - KL(q_{\\phi}(z) \\ || \\ p_{\\theta}(z|x)) \\ (1) \\\\  L(\\theta, \\phi) &= E_{Z \\sim q_{\\phi}}[log \\ p_{\\theta}(x,z) - log \\ q_{\\phi}(z|x)] \\ (2)\\end{align}$$\n",
    "\n",
    "Now, at each optimization step of the ELBO, we can break it down into two separate steps where each parameter is considered constant in turn:\n",
    "\n",
    "1. Maximize (1) with respect to $\\phi$ to minimize the KL divergence.\n",
    "2. Maximize (2) with respect to $\\theta$ to maximize the likelihood of our data.\n",
    "\n",
    "Hence, our objective is:\n",
    "\n",
    "$$\\hat{\\phi}, \\hat{\\theta} = argmax_{\\phi, \\theta} \\ L(\\phi, \\theta)$$\n",
    "\n",
    "Now that we have formulated the optimization problem, we need to find a solution. However, the presence of the expectation in the ELBO makes this task challenging, leading to a similar issue encountered in the Reparameterization Trick. To address this, we will employ the reparameterization trick along with the Monte Carlo method to compute gradients for stochastic gradient ascent. Suppose we have n data points, and for each data point, we will sample L times from a normal distribution $N(0, I)$ since in VAEs, $q_{\\phi}$ is a normal distribution (as explained in the Reparameterization Trick section).\n",
    "\n",
    "\n",
    "$$\\begin{align} \\epsilon_{i,l} &\\sim N(0,I) \\\\ g_{\\phi}(x_i, \\epsilon_{i,l}) &= h_{\\phi}^{(1)}(x) + h_{\\phi}^{(2)}(x) \\ \\epsilon_{i,l} = z_i\\end{align}$$\n",
    "\n",
    "Recall $h_{\\phi}^1(x) = \\mu$ and $h_{\\phi}^2(x) = \\sigma^2$. Using the trick and Monte Carlo method we can calculate following gradient(requirenments for differentiation are satisfied):\n",
    "\n",
    "$$\\nabla_{\\phi, \\theta} \\hat{L(\\phi, \\theta)} = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{L}\\sum_{l=1}^L \\nabla_{\\phi, \\theta} [log \\ p_{\\theta}(x_i,g_{\\phi}(x_i, \\epsilon_{i,l}) - log \\ q_{\\phi}(g_{\\phi}(x_i, \\epsilon_{i,l}) | x_i))]$$\n",
    "\n",
    "We have completed the definition of a loss function and how to minimize it to obtain $\\hat{\\phi}$ and $\\hat{\\theta}$. However, we can still capitalize on our model distribution. This means we can rephrase the ELBO in a different manner that helps reduce the variance of gradients. It is important to note that the KL divergence has an analytical form when both distributions are normal (now the reason why we defined $p(z)$ and $p(x|z)$ as Gaussian becomes evident). First, let's express the ELBO in terms of $p_{\\theta}(z)$:\n",
    "\n",
    "$$\\begin{align} L(\\phi, \\theta) &= \\sum_{i=1}^nE_{Z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ (p_{\\theta}(x_i|z_i)p_{\\theta}(z_i)) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(x_i|z_i) + log \\ p_{\\theta}(z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &=^* \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] + \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] + \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ \\frac{p_{\\theta}(z_i)}{ q_{\\phi}(z_i|x_i)}] \\\\ &= \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))\\end{align}$$\n",
    "\n",
    "The analytical form is:\n",
    "$$KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i)) = -\\frac{1}{2}\\sum_{j=1}^J 1 + log(\\sigma^2(x_i)_j) - (\\mu(x_i)_j)^2 - \\sigma^2(x_i)_j$$\n",
    "\n",
    "Therefore, the final ELBO after using analytical form and reparameterization trick and also Monte Carlo trick for the first term is:\n",
    "$$\\hat{L(\\phi, \\theta)} = \\frac{1}{n}\\sum_{i=1}^n [\\frac{1}{2} \\sum_{j=1}^J (1 + log(h_{\\phi}^{(2)}(x_i)_j) - (h_{\\phi}^{(1)}(x_i)_j)^2 - h_{\\phi}^{(2)}(x_i)_j) + \\frac{1}{L}\\sum_{l=1}^L[logp_{\\theta}(x_i|g_{\\phi}(x_i, \\epsilon_{i,l}))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6de243-f28e-4506-bc1e-2003d2a72e22",
   "metadata": {},
   "source": [
    "<a name='3.1.5'></a>\n",
    "#### The autoencoder perspective\n",
    "The process of VAEs is quite similar to autoencoders. In autoencoders, the encoder network transforms input data into a lower-dimensional latent space, while the decoder network maps the latent code back to the original data space. However, there is a key distinction between them: in autoencoders, an input $x$ is encoded as a single point in the latent space, whereas in VAEs, an input $x$ is encoded as a distribution from which $z$ is sampled. Thus, VAEs can be considered as a probabilistic version of autoencoders. By introducing randomness in the latent space, VAEs facilitate the use of a diverse range of $z$ values in generating $x$. This stochasticity in the sampling process results in a more versatile and adaptable generative model.\n",
    "\n",
    "Another significant difference between the two lies in regularization. In VAEs, in addition to architectural aspects, an essential component involves the regularization applied to the latent code. In a VAE, the latent code is subjected to regularization through a Gaussian distribution with a predefined mean and variance. This regularization serves the purpose of preventing overfitting by encouraging a smooth distribution for the latent code, thereby discouraging it from memorizing the training data entirely.\n",
    "\n",
    "Moreover, this regularization plays a crucial role in enabling VAEs to create new data samples that smoothly interpolate between existing training data points. This ability enhances VAEs' capacity to generate new data samples that exhibit patterns similar to those in the training data. Additionally, the regularization in VAEs prevents the decoder network from merely reproducing the input data accurately. Instead, it prompts the decoder network to learn a more generalized representation of the data, thereby improving the VAE's effectiveness in generating diverse data samples.\n",
    "\n",
    "Let's verify the concept mathematically. Let $h(\\phi)$ be the encoder function and $f(\\theta)$ be the decoder function:\n",
    "\n",
    "\n",
    "$$J_{AE}(\\phi, \\theta) = \\sum_{i=1}^n ||x_i - f_{\\theta}(h_{\\phi}(x_i))||_2^2$$\n",
    "$$J_{VAE}(\\phi, \\theta) = - ELBO(\\phi, \\theta) = -\\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log(p_{\\theta}(x_i|z_i))] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))$$\n",
    "\n",
    "We assume $p_{\\theta}(x_i,z_i)$ in VAEs is a normal distribution like $N(\\mu_d,\\sigma_dI)$. So we can write $logp_{\\theta}(x_i|z_i)$ in a way that the squared error appears:\n",
    "$$J_{VAE}(\\phi, \\theta) = -\\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[\\frac{-||x_i - f_{\\theta}(h_{\\phi}(x_i))||_2^2}{2\\sigma_d^2} \\ log(\\frac{1}{\\sqrt{2\\pi\\sigma_d^2}})] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))$$\n",
    "\n",
    "Therefore, both AE and VAE aim to minimize the squared loss error. This error is known as the reconstruction loss as it aims to penalize the disparity between the original input and the reconstructed input. However, in the cost function of VAE, we also have KL divergence, which is the distinguishing factor. This term functions as a regularization term by preventing the model from generating latent variables from any arbitrary distribution (in the case of AE, one extreme instance is where each data point in the dataset is mapped to a number and decoded in the reverse direction). This regularization encourages the learned latent variables to exhibit distributions similar to the prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975187d1-6a11-41d8-9805-6e3bbdc49815",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1640b0-c0eb-4ed5-86b2-7d1347a04082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd732db-40d8-4e67-a3be-2c9e39e8bfe2",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### Dimension Reduction using Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ac19e-fa7f-4e7b-91d4-d7ff56e8fd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b4afd3-f9a8-431b-85d0-9c73a8c8e2cc",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### Hand-written Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4336f0c-67ce-4111-aced-0ab3a33c4941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
