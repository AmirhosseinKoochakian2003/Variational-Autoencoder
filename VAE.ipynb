{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b8be54-9eea-4900-8d39-4d9545cbaef2",
   "metadata": {},
   "source": [
    "# Variational autoencoders\n",
    "\n",
    "In this project, we will explore the idea of variational autoencoders. Also, we will implement a variational autoencoder using Tensorflow to generate images like MNIST hand-written digits.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Introduction](#1)\n",
    "- [2 - Autoencoders](#2)\n",
    "    - [2.1 - Dimension Reduction using Autoencoders](#2.1)\n",
    "- [3 - Variational autoencoders](#3)\n",
    "    - [3.1 - Theory behind VAEs](#3.1)\n",
    "        - [3.1.1 - Evidence Lower BOund](#3.1.1)\n",
    "        - [3.1.2 - Variational Inference](#3.1.2)\n",
    "        - [3.1.3 - Reparameterization Trick](#3.1.3)\n",
    "        - [3.1.4 - The probability model perspective](#3.1.4)\n",
    "        - [3.1.5 - The autoencoder perspective](#3.1.5)\n",
    "        - [3.1.6 - Hand-written Image Generator](#3.1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47500df-22a0-46de-9ff1-5378782bcb7d",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Introduction\n",
    "Variational autoencoders (VAEs) are a type of deep generative models that encode input data into a lower-dimensional space, randomly sample points from this latent space, and then decode these points to generate new data. More precisely, VAEs can be viewed in two different ways:\n",
    "\n",
    "1. VAEs are a type of autoencoder with a key distinction from traditional autoencoders (AEs), which will be explained later when we discuss variational autoencoders.\n",
    "2. VAEs are probabilistic generative models that operate on the joint distribution of observed and latent variables.\n",
    "\n",
    "In this notebook, we will provide a brief overview of autoencoders, variational autoencoders, delve into the underlying mathematics, and demonstrate the implementation of both an autoencoder and a variational autoencoder with the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ffa3d-ba6c-4eff-b2da-d8245aaa129a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## Autoencoders\n",
    "An autoencoder is a type of neural network used to reduce high-dimensional data to a lower-dimensional representation. It consists of three parts: encoder, bottleneck, and decoder. The encoder compresses the data to a lower-dimensional form, the bottleneck stores the crucial information, and the decoder reconstructs the output from the bottleneck. During training, the output is compared to the original input to calculate reconstruction loss, which is the squared error between the input and output. Essentially, training an autoencoder involves optimizing the encoder and decoder to minimize the reconstruction error.\n",
    "\n",
    "Applications of autoencoders include:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Image denoising\n",
    "- Anomaly detection\n",
    "\n",
    "| <img src=\"https://miro.medium.com/v2/resize:fit:600/0*83pAHsVsnsqySOHN.png\" width=\"400\" height=\"300\"> | \n",
    "|:--:| \n",
    "| *Autoencoder architecture* |\n",
    "*source : [Medium](https://medium.com/@soumallya160/the-basic-concept-of-autoencoder-the-self-supervised-deep-learning-454e75d93a04)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cca20-c2e9-40b7-907a-ce993b22b47a",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### Dimension Reduction using Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca474c4-4ae8-424f-8ce5-b397602af844",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## Variational autoencoders\n",
    "\n",
    "A variational autoencoder (VAE) is a type of probabilistic autoencoder designed to model the distribution of data. They share a similar architecture with standard autoencoders, featuring an encoder, bottleneck, and decoder. However, the way information is processed in the bottleneck of a VAE differs from that of a traditional autoencoder.\n",
    "\n",
    "VAEs can be understood in two main ways:\n",
    "\n",
    "1. Probabilistic Generative Model: VAEs function as probabilistic generative models, employing a Bayesian approach to learn the probability distribution of input data.\n",
    "2. Autoencoder: While VAEs belong to the family of autoencoders, they differ in that they are probabilistic rather than deterministic. In a VAE, the compressed representation (latent vector ùëß) is not directly learned but is generated randomly based on the learned distribution.\n",
    "\n",
    "\n",
    "| <img src=\"data:image/png;base64,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\" width=\"400\" height=\"300\"> | \n",
    "|:--:| \n",
    "| *Variational autoencoder architecture* |\n",
    "*source : [Medium](https://www.google.com/imgres?imgurl=https%3A%2F%2Fmiro.medium.com%2Fv2%2Fresize%3Afit%3A1358%2F0*2DZwupQZTnpBEH1s.png&tbnid=6LvLReF4DrHNNM&vet=12ahUKEwig6JCg5rKEAxX4BfsDHW-xCewQMygSegQIARB0..i&imgrefurl=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fvariational-auto-encoders-vae-for-the-numerai-dataset-2709dcc7e449&docid=Zkc9F34LppKCyM&w=1264&h=524&q=variational%20auto%20encoder&ved=2ahUKEwig6JCg5rKEAxX4BfsDHW-xCewQMygSegQIARB0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014ece3-03c2-4c6f-bbf2-1f4c19f769d3",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### Theory behind VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a4cc-ee4b-4170-aa13-81907d806299",
   "metadata": {},
   "source": [
    "<a name='3.1.1'></a>\n",
    "#### Evidence Lower BOund\n",
    "The Evidence Lower Bound (ELBO) plays a crucial role in approximating the posterior distribution. It serves as a minimum value for the logarithm of the total likelihood of the data given the model parameters. By maximizing the ELBO, we effectively minimize the Kullback-Leibler (KL) divergence between the actual posterior distribution and the approximated one.\n",
    "\n",
    "Before delving into the lower bound concept, let's lay out the problem. Let's assume we have two random variables: $X$ (observed or real data) and $Z$ (hidden or latent variable). Both $X$ and $Z$ follow a joint distribution denoted as $p(x,z;\\theta)$, where $\\theta$ represents the distribution's parameters.\n",
    "\n",
    "Now, what exactly is the evidence? The evidence simply refers to the log-likelihood of observations ùë• under a fixed $\\theta$. Essentially, the likelihood function indicates how well our model and parameter $\\theta$ match the observations. A high likelihood value suggests that the model fits the data well. Our objective now is to find a lower bound for $p(x;\\theta)$. Let's assume that $z$ follows a distribution denoted as ùëû. We can leverage a technique called marginalization to derive the desired lower bound.\n",
    "\n",
    "$$\\begin{align} log p(x;\\theta) &= log\\int_{z} p(x,z;\\theta) dz \\\\ &= log\\int_{z} p(x,z;\\theta) \\frac{q(z)}{q(z)} dz \\\\ &= log E_{z\\sim q}[\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ & \\geq^* E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}]\\end{align}$$\n",
    "\n",
    "$$\\begin{align} => ELBO = E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}] \\ (1) \\end{align}$$\n",
    "\n",
    "\\* : This inequality is the result of [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality). Since $log$ is a concave function, $log(E[X]) \\geq E[log(X)]$ where X in this statement is $\\frac{p(x,Z;\\theta)}{q(Z)}$.\n",
    "\n",
    "Now, let's explain why maximizing the Evidence Lower Bound (ELBO) leads to minimizing the Kullback-Leibler (KL) divergence between the true posterior distribution and the approximating distribution. Let's assume we aim to identify a distribution, denoted as $q$, that accurately approximates $p(z|x; \\theta)$. In the context of Variational Autoencoders (VAEs), our goal is to approximate $p(z|x; \\theta)$. This requires us to find a distribution that closely approximates it, which is why I am discussing this specific distribution.\n",
    "\n",
    "$$\\begin{align} KL \\ (q(z) \\ || \\ p(z|x;\\theta)) &= E_{Z \\sim q}[log \\frac{q(Z)}{p(Z|x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z|x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\frac {p(Z,x;\\theta)}{p(x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z,x;\\theta)] + E_{Z \\sim q}[log \\ p(x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ p(x;\\theta)] - E_{Z \\sim q}[log\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ &=^{*,(1)} log \\ p(x;\\theta) - ELBO \\\\ &= evidence - ELBO\\end{align}$$\n",
    "\n",
    "\\* : Notice that  $log p(x;\\theta)$ is not dependent on  $z$, so it behaves like a constant and can be taken out of the expectation.\n",
    "\n",
    "Now, consider that when $\\theta$ is fixed and we are searching for a distribution q that minimizes the Kullback-Leibler (KL) divergence, the evidence term acts as a constant. Therefore, due to the negative sign of the Evidence Lower Bound (ELBO), maximizing the ELBO results in minimizing the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff2f0a-dde6-44d0-a97b-16a728f15c53",
   "metadata": {},
   "source": [
    "<a name='3.1.2'></a>\n",
    "#### Variational Inference\n",
    "\n",
    "In many real-world situations, it's impractical to calculate the exact probability distribution due to either complex models or large datasets. Variational inference offers a method to estimate this distribution by selecting a simpler distribution from a set of parameterized distributions. The concept of variational inference involves treating the task of approximating the probability distribution as an optimization challenge. The objective is to identify the distribution within the chosen set that minimizes the difference, measured by Kullback-Leibler divergence, from the true distribution.\n",
    "\n",
    "By maximizing the Evidence Lower Bound (ELBO), a surrogate for the log likelihood of the data based on the model parameters, variational inference aims to find the most precise approximation to the actual probability distribution given the model and observed data.\n",
    "\n",
    "When dealing with models containing hidden ($Z$) and observed ($X$) variables and trying to determine the likelihood of $Z$ given $X$, variational inference aids in finding a suitable estimation. However, let's first understand why we cannot directly calculate  $p(z|x)$ . By applying Bayes' theorem,  $p(z|x)$  can be expressed as follows:\n",
    "\n",
    "$$\\begin{align} p(z|x) = \\frac{p(z,x)}{p(x)} = \\frac{p(x|z) \\times p(z)}{\\int_z p(z,x) dz} \\end{align}$$\n",
    "\n",
    "The key thing to note is that computing the denominator is not always possible. Therefore, variational inference is utilized.\n",
    "\n",
    "In this method, we select a group of distributions called the variational distribution family and strive to discover a $q$ that comes closest to  $p(z|x)$ within this family. Let's also assume that $\\phi$ controls these distributions (it's the variational parameter). Hence, our aim is to identify a $\\phi$ that minimizes the KL divergence between  $q_{\\phi}(z)$ and  $p(z|x)$ . Interestingly, as indicated by the [Evidence Lower BOund](#3.1.1), instead of minimizing the KL divergence, we can maximize the ELBO to find the most accurate approximation. Therefore, we must now handle $\\phi$ as it is a parameter that characterizes the family of distributions.\n",
    "\n",
    "$$\\begin{align} ELBO(\\phi) &= L(\\phi) = E_{Z \\sim q_{\\phi}}[log \\ p(x,Z) - log \\ q(Z)] \\\\ \\hat{\\phi} &= argmax_{\\phi} \\ L(\\phi) \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5996d0c-b686-480d-9cbf-f5d271642e53",
   "metadata": {},
   "source": [
    "<a name='3.1.3'></a>\n",
    "#### Reparameterization Trick\n",
    "\n",
    "In a typical variational inference scenario, our goal is to estimate the distribution of hidden variables using a variational distribution. When the variational distribution involves sampling from a distribution like a Gaussian, it introduces randomness into the calculation of gradients during the optimization procedure. The reparameterization trick helps to tackle this problem by modifying the way the random variable is represented so that the randomness is isolated from the distribution's parameters. This allows gradient-based optimization methods to navigate through the random sampling process, resulting in more consistent training with lower variance gradient estimations. Therefore, this approach enables us to conduct stochastic gradient ascent on the ELBO. Now that we understand how we approximate the hidden variable distribution, we will employ optimization techniques to determine the optimal parameters in our scenario.\n",
    "\n",
    "##### **High Variance Gradients**\n",
    "First lets explore what happens if we do not use reparameterization trick. Let‚Äôs say we want to take the gradient w.r.t. $\\theta$ of the $E_{x \\sim p}[f_{\\theta}(x)]$. If the p distribution is not parameterized by $\\theta$, then the gradient can easily go inside of the expectation. However, if it is not the case, then we can not calculate the gradient in general.\n",
    "\n",
    "First, let's consider what happens if we don't employ the reparameterization trick. For instance, suppose we aim to compute the gradient with respect to the parameter $\\theta$ of the expectation $E_{x \\sim p}[f_{\\theta}(x)]$. If the distribution $p$ is not determined by $$, then the gradient can straightforwardly be incorporated within the expectation. Nevertheless, if this is not the scenario, then in general, we are unable to compute the gradient.\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\theta} \\ E_{x \\sim p_{\\theta}}[f_{\\theta}(x)] &= \\nabla_{\\theta} \\int_{x} p_{\\theta}(x) f_{\\theta}(x) dx \\\\ &= \\int_{x}\\nabla_{\\theta} ( p_{\\theta}(x) f_{\\theta}(x)) dx \\\\ &= \\int_{x}\\nabla_{\\theta}p_{\\theta}(x) f_{\\theta}(x)dx + \\int_{x} p_{\\theta}(x) \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &= \\int_{x}\\nabla_{\\theta}p_{\\theta}(x) f_{\\theta}(x)dx + E_{x \\sim p_{\\theta}}[\\nabla_{\\theta}f_{\\theta}(x)]\\end{align} $$\n",
    "\n",
    "The first term in the final expression is not guaranteed to be an expectation. While Monte Carlo methods necessitate the ability to sample from $p(x)$, they do not inherently require access to its gradient information. This limitation becomes challenging when dealing with scenarios where computing the gradient of $p(x)$ with respect to $\\theta$ analytically is infeasible, which is often the case in practice. Therefore, in order to use Monte Carlo methods, we need to apply changes that remove $\\nabla_{\\theta} \\ p_{\\theta}(x)$ from our integrals.\n",
    "\n",
    "##### **Reparameterization Trick Explanation**\n",
    "Sometimes the random variable $x \\sim p_{\\theta}(x)$ can be reparameterized as a deterministic function $g$ of $\\theta$ and of a random variable $\\epsilon \\sim D$, where $D$ is not parameterized on $\\theta$.\n",
    "$$\\begin{align} \\epsilon &\\sim D \\\\ x &= g_{\\theta}(\\epsilon) \\end{align}$$\n",
    "\n",
    "Now lets see what happen if we take gradient from $E_{x \\sim p}[f_{\\theta}(x)]$:\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\theta} \\ E_{x \\sim p_{\\theta}}[f_{\\theta}(x)] &= \\nabla_{\\theta} \\ E_{\\epsilon \\sim g_{\\theta}}[f(g_{\\theta}(\\epsilon))] \\\\ &=^* E_{\\epsilon \\sim g_{\\theta}}[\\nabla_{\\theta} f(g_{\\theta}(\\epsilon))]\\end{align} $$\n",
    "\n",
    "\\* : Notice that $p(\\epsilon)$ or more generally $D$ are not dependent to $\\theta$, so we can move the gradient inside the expectation.\n",
    "\n",
    "As we showed, now we can use Monte Carlo method to estimate this expectation.\n",
    "\n",
    "One may ask that how how the $D$ distribution can be found? Generally it is not a simple problem, but some family of distributions conduct to simple answers. For example, in our case we are dealing with Guassian distributions($q(z|x)$) so we can choose standard normal distribution and our $g$ function as: $g_{\\theta}(\\epsilon) = \\mu + \\sigma \\epsilon$.\n",
    "\n",
    "In order to use reparameterization trick we should consider these requirenments as well:\n",
    "- $f_{\\theta}(x)$ must be differentiable w.r.t x its input.\n",
    "- $g_{\\theta}(\\epsilon)$ must exist and be differentiable w.r.t. $\\theta$.\n",
    "\n",
    "##### **Stochastic Gradient Ascent of the ELBO**\n",
    "For this part we will apply following idea on our original problem which was discussed in [Evidence Lower BOund](#3.1.1) and [Variational Inference](#3.1.2), so all notations and symbols are with respect to these parts.\n",
    "\n",
    "The goal of using variational inference is to find the closest $q$ to $p(z|x)$ using continuous optimization techniques like gradient ascent. Consequently, at each step, parameters are updated in this way:\n",
    "\n",
    "$$\\begin{align} \\phi_{t+1} = \\phi_t + \\alpha \\ \\nabla_{\\phi} \\ L(\\phi) \\end{align}$$\n",
    "Notic that we are trying to maximize the ELBO, so gradient ascent is used.\n",
    "\n",
    "As we saw before, ELBO consists of an expectation, so the gradients face randomness in back propagation. In order to get rid of this randomness, we can use reparametrization trick. Before exploring this idea, we need to check another problem and it is how can gradients be calculated? As we can see, what makes our computation complex is dealing with integrals. Therefore, we use stochastic gradient descent that consider a distribution for gradients and at each step, sample a gradient from that distribution to update the $\\phi$. Instead of computing the exact gradient of the ELBO with respect to $\\phi$, we formulate a random variable $V(\\phi)$ that holds $E[V(\\phi)] = \\nabla_{\\phi} L(\\phi)$.(it is just a formal way of expressing the SGD). \n",
    "$$\\begin{align} v &\\sim V(\\phi) \\\\ \\phi_{t+1} &= \\phi_{t} + \\alpha \\ v \\end{align}$$\n",
    "\n",
    "First, we will reparameterize $q_{\\phi}(z)$ in the following way:\n",
    "$$\\begin{align} \\epsilon &\\sim N(0,1) \\\\ z &\\sim g_{\\phi}(\\epsilon) \\end{align}$$\n",
    "\n",
    "Where $g$ has the form that we talked in previous section.\n",
    "\n",
    "Now we replace every $z$ in ELBO equation with $g_{\\phi}$:\n",
    "$$\\begin{align} L(\\phi) = E_{\\epsilon \\sim N(0,1)}[log \\ p(x,g_{\\phi}(\\epsilon)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon))]\\end{align}$$\n",
    "\n",
    "As we showed previously, this reparameterization enables us to approximate the ELBO via Monte Carlo sampling. So we will sample L times from standard normal distribution, denote ith sample as $\\epsilon_i$ and estimate the expectation as:\n",
    "$$\\hat{L(\\phi)} = \\frac{1}{L} \\sum_{l=0}^{L}[log \\ p(x,g_{\\phi}(\\epsilon_l)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon_l))]$$\n",
    "\n",
    "Because of all requirenments are satisfied, we can take gradient from both sides:\n",
    "$$\\nabla_{\\phi} \\ \\hat{L(\\phi)} = \\nabla_{\\phi} \\frac{1}{L} \\sum_{l=0}^{L}[log \\ p(x,g_{\\phi}(\\epsilon_l)) - log \\ q_{\\phi}(g_{\\phi}(\\epsilon_l))]$$\n",
    "It is easy to show that $E[\\nabla_{\\phi} \\ \\hat{L(\\phi)}] = \\nabla_{\\phi} \\ L(\\phi)$.(just needs to bring the $\\nabla_{\\phi}$ out of expectation and move the expectation inside the sigma). Therefore, $E[\\nabla_{\\phi} \\ \\hat{L(\\phi)}]$ is what we were looking for.\n",
    "$$V(\\phi)= \\nabla_{\\phi} \\ \\hat{L(\\phi)}$$\n",
    "In simpler terms,  sampling $\\epsilon_1,\\dots,\\epsilon_L$ from $N(0,1)$, computing an approximate ELBO, and finding the gradient to this approximation is basically like drawing random gradients from a distribution $V(\\phi)$ where the average gradient matches the ELBO gradient. This is why it's important for $D$ to be easy to sample from when using the reparametrization trick: we rely on samples from $D$ to create samples for $V(\\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdb1e5-05bf-4098-9859-9c4361c3f162",
   "metadata": {},
   "source": [
    "<a name='3.1.4'></a>\n",
    "#### The probability model perspective\n",
    "Now we have all neccessary tools to show full theory behind the VAEs. In this section we will talk about probabilistic aspect of VAEs. \n",
    "\n",
    "Variational autoencoders (VAEs) function as generative models by defining a joint distribution over samples and their associated latent variable, denoted as $p(x,z)$. To delve into this, let's define the key components:\n",
    "- Latent Variable(Z) : In VAEs, a latent variable $z$ is crucial for representing complex data. The model relies on a simplified conditional distribution $p(x,z)$, where $x$ denotes observable data, and $z$ remains latent. This distribution is expressed as $p(x,z)=p(x‚à£z)p(z)$, reflecting the likelihood of observed data given the latent variable and the distribution of the latent variable itself. This approach empowers VAEs to capture intricate patterns and generate meaningful data representations.\n",
    "- Observation Variable(X) : The variable $x$ in VAEs represents observed data, and its distribution is characterized by $p(x‚à£z)$, denoting the likelihood of observing $x$ given a latent variable $z$. This likelihood distribution guides the VAE framework in learning and generating meaningful representations of input data, with $z$ capturing hidden patterns and enabling diverse sample generation during training.\n",
    "\n",
    "The generative process is as follows:\n",
    "\n",
    "1.Sample a latent variable $z$ from a prior distribution, typically $N(0,I)$.\n",
    "\n",
    "2.Map $z$ to parameters of another distribution to sample $x$ using $p(x‚à£z)$.\n",
    "\n",
    "Notice that in VAEs, both distributions are considered as normal distributions. Right now we can calculate the $p(x,z)$ using $p(z)$ and $p(x|z)$ while assuming we have a good $\\theta$ parameter for them, but as i explained, $Z$ is a latent or hidden variable, so we do not have any information about it in our dataset. Therefore, in Variational Autoencoders (VAEs), the primary objective is to approximate the posterior distribution $p(z‚à£x)$ rather than the latent variable's prior $p(z)$ or the likelihood $p(x‚à£z)$. The unobservable nature of the latent variable $z$ in the dataset prompts the interest in determining the posterior distribution for each example in the data, representing the model's response to observed data. By focusing on $p(z‚à£x)$, VAEs adapt the latent space to the specific characteristics of the observed data, incorporating information, handling variability, and optimizing for effective reconstruction. This data-dependent approach aligns with the generative process and improves representation learning, ensuring that the latent space captures meaningful patterns for enhanced generative capabilities. While finding the $p(z|x)$, we can find $p(x)$, So our goals in VAEs are:\n",
    "\n",
    "1.For fixed $\\theta$, compute $p_{\\theta}(z_i|x_i)$ for each $x_i$.\n",
    "\n",
    "2.Maximize likelihood estimation (MLE) of $\\theta$.\n",
    "\n",
    "But we can not calculate these two without approximation because in order to compute these two, we need to marginalize out the $z$ in $p(x,z)$ and this means that an integral over all latent space is required which is intracitble.(I mentioned the problem more percisely in [variational inference](#3.1.2)). So in order to find an approximation solutions to these problems, variational inference is used. First, like previous parts, assume that $\\theta$ is fixed. To approximate $p_{\\theta}(z_i|x_i)$, we can use [variational inference](#3.1.2) and \n",
    "[evidence lower bound](#3.1.1). VI performs approximating by choosing a family of distributions and then finds the closest distribution to $p_{\\theta}(z_i|x_i)$. \n",
    "\n",
    "Before defining our variational family, lets call the function that calculate the mean and standard deviation of x, $h_{\\phi}$.($\\phi$ is the parameter of the h function.) Note that in variational autoencoders this h is a neural network.\n",
    "$$\\begin{align} \\mu &= h_{\\phi}^1(x) \\\\ \\sigma^2 &= h_{\\phi}^2(x) \\end{align}$$\n",
    "\n",
    "$$variational \\ family = \\{q_{\\phi} | q_{\\phi} \\ \\sim N(h_{\\phi}^1(x), h_{\\phi}^2(x))\\}$$\n",
    "\n",
    "\n",
    "In order to have an loss function to measure how much these two distributions are close to each other, we use Kullback‚ÄìLeibler divergence between $q_{\\phi}(z_i|x_i) \\in variational \\ family$(approximation distribution) and $p_{\\theta}(z_i|x_i)$. Now we know that minimizing the KL divergence is equivalent to maximizing the ELBO.\n",
    "$$\\begin{align} L(\\phi) &= E_{Z_1,\\dots,Z_n \\sim q_{\\phi}}\\sum_{i=1}^n[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] \\\\ &= \\sum_{i=1}^nE_{Z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)]\\end{align}$$\n",
    "\n",
    "So far we have assumed that $\\theta$ is fixed and we did not optimize the ELBO w.r.t $\\theta$. But $\\theta$ is the parameter of the decoder function and we have to optimize decoder as well as encoder. Therefore, we can extend the definition of our loss function and consider ELBO as function of both $\\phi$ and $\\theta$. Now to goal is to find both $\\phi$ and $\\theta$ that maximize the ELBO. We can show that why it is true in the following way: we know $L(\\theta, \\phi) = evidence(\\theta) - KL(q_{\\phi}(z) \\ || \\ p_{\\theta}(z|x))$ and ELBO is a lower bound for log likelihood, so if we maximize the ELBO with respect to both $\\phi$ and $\\theta$, then the evidence will increase as much as ELBO and we can conclude that KL divergence is decreasing or being the same. Another way to justify this is we can consider the process of gradient ascent separately for $\\phi$ and $\\theta$. Consider this two forms for ELBO:\n",
    "$$\\begin{align} L(\\theta, \\phi) &=  log \\ p_{\\theta}(x) - KL(q_{\\phi}(z) \\ || \\ p_{\\theta}(z|x)) \\ (1) \\\\  L(\\theta, \\phi) &= E_{Z \\sim q_{\\phi}}[log \\ p_{\\theta}(x,z) - log \\ q_{\\phi}(z|x)] \\ (2)\\end{align}$$\n",
    "\n",
    "Now every step of maximization of the ELBO can be considered as two separate step(at each step, the other parameter will be considered as constant):\n",
    "- maximize (1) with respect to $\\phi$. This leads to minimizing the KL divergence.\n",
    "- maximize (2) with respect to $\\theta$. This leads to maximizing the likelihood of our data.\n",
    "\n",
    "Therefore, our objective is:\n",
    "$$\\hat{\\phi}, \\hat{\\theta} = argmax_{\\phi, \\theta} \\ L(\\phi, \\theta)$$\n",
    "\n",
    "Now that we‚Äôve set up the optimization problem, we need to solve it. Unfortunately, the expectation present in the ELBO makes this difficult and cause the same problem that we saw in [Reparameterization Trick](#3.1.3), so we will use reparameterization trick followed by Monte Carlo method to calculate the gradients for stochastic gradient ascent. Assume we have n data points and for each data point we will sample L times from $N(0,I)$ because in VAEs, $q_{\\phi}$ is a normal distribution. (similar to what i explained in Reparameterization trick section).\n",
    "\n",
    "$$\\begin{align} \\epsilon_{i,l} &\\sim N(0,I) \\\\ g_{\\phi}(x_i, \\epsilon_{i,l}) &= h_{\\phi}^1(x) + h_{\\phi}^2(x) \\ \\epsilon_{i,l} = z_i\\end{align}$$\n",
    "\n",
    "Recall $h_{\\phi}^1(x) = \\mu$ and $h_{\\phi}^2(x) = \\sigma^2$. Using the trick and Monte Carlo method we can calculate following gradient(requirenments for differentiation are satisfied):\n",
    "\n",
    "$$\\nabla_{\\phi, \\theta} \\hat{L(\\phi, \\theta)} = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{L}\\sum_{l=1}^L \\nabla_{\\phi, \\theta} [log \\ p_{\\theta}(x_i,g_{\\phi}(x_i, \\epsilon_{i,l}) - log \\ q_{\\phi}(g_{\\phi}(x_i, \\epsilon_{i,l}) | x_i))]$$\n",
    "\n",
    "We've finished defining a loss function and how to minimize(actually maximize) it to achieve a $\\hat{\\phi}$ and $\\hat{\\theta}$. But we still can take advantage from our model distribution and by this i mean we can rewrite the ELBO in another way that leads to reduce the variance of gradients. What is neccessary to pay attention is KL divergence has a analytical form when both distributions are normal(now one of the reason why we defined $p(z)$ and $p(x|z)$ as Gaussian becomes clear.). First lets write ELBO in terms of $p_{\\theta}(z)$:\n",
    "$$\\begin{align} L(\\phi, \\theta) &= \\sum_{i=1}^nE_{Z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(x_i,z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ (p_{\\theta}(x_i|z_i)p_{\\theta}(z_i)) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(x_i|z_i) + log \\ p_{\\theta}(z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &=^* \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] + \\sum_{i=1}^n\\int q_{\\phi}(z_i|x_i)[log \\ p_{\\theta}(z_i) - log \\ q_{\\phi}(z_i|x_i)] dz_i \\\\ &= \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] + \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ \\frac{p_{\\theta}(z_i)}{ q_{\\phi}(z_i|x_i)}] \\\\ &= \\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log \\ p_{\\theta}(x_i|z_i)] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))\\end{align}$$\n",
    "\n",
    "The analytical form is:\n",
    "$$KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i)) = -\\frac{1}{2}\\sum_{j=1}^J 1 + log(\\sigma^2(x_i)_j) - (\\mu(x_i)_j)^2 - \\sigma^2(x_i)_j$$\n",
    "\n",
    "Therefore, the final ELBO after using analytical form and reparameterization trick and also Monte Carlo trick for the first term is:\n",
    "$$\\hat{L(\\phi, \\theta)} = \\frac{1}{n}\\sum_{i=1}^n [\\frac{1}{2} \\sum_{j=1}^J (1 + log(h_{\\phi}^{(2)}(x_i)_j) - (h_{\\phi}^{(1)}(x_i)_j)^2 - h_{\\phi}^{(2)}(x_i)_j) + \\frac{1}{L}\\sum_{l=1}^L[logp_{\\theta}(x_i|g_{\\phi}(x_i, \\epsilon_{i,l}))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6de243-f28e-4506-bc1e-2003d2a72e22",
   "metadata": {},
   "source": [
    "<a name='3.1.5'></a>\n",
    "#### The autoencoder perspective\n",
    "The process of VAEs is so similar to autoencoders, the encoder network maps the input data to a lower-dimensional latent space, and the decoder network maps the latent code back to the original data space. However there is a key difference between them: in autoencoders, an input $x$ is encoded as a single point in latent space while in VAEs, an input $x$ is encoded as a distribution and $z$ is sampled from this distribution. Therefore, VAEs are probabilistic version of autoencoders. By introducing randomness in the latent space, VAEs allow for a diverse set of $z$ values to contribute to the generation of $x$. This stochasticity during the sampling process leads to a more expressive and flexible generative model.\n",
    "\n",
    "There is also another key difference between these two: Regularization. In VAEs, aside from the architectural considerations, a crucial element involves the regularization applied to the latent code. Within a VAE, the latent code undergoes regularization through a Gaussian distribution featuring a fixed mean and variance. This regularization serves the purpose of preventing overfitting by promoting a smooth distribution for the latent code, discouraging it from simply memorizing the training data.\n",
    "\n",
    "Furthermore, this regularization plays a pivotal role in enabling VAEs to generate novel data samples that smoothly interpolate between existing training data points. This characteristic enhances VAEs' capability to produce new data samples resembling the patterns seen in the training data. Additionally, the regularization in VAEs serves to prevent the decoder network from precisely reconstructing the input data. Instead, it compels the decoder network to acquire a more generalized representation of the data, thereby enhancing the VAE's efficacy in generating diverse data samples.\n",
    "\n",
    "Lets check the idea mathematically, $h_{\\phi}$ is the encoder function and $f_{\\theta}$ is the decoder function:\n",
    "\n",
    "$$J_{AE}(\\phi, \\theta) = \\sum_{i=1}^n ||x_i - f_{\\theta}(h_{\\phi}(x_i))||_2^2$$\n",
    "$$J_{VAE}(\\phi, \\theta) = -\\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[log(p_{\\theta}(x_i|z_i))] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))$$\n",
    "\n",
    "We assume $p_{\\theta}(x_i,z_i)$ in VAEs is a normal distribution like $N(\\mu_d,\\sigma_dI)$. So we can write $logp_{\\theta}(x_i|z_i)$ in a way that the squared error appears:\n",
    "$$J_{VAE}(\\phi, \\theta) = -\\sum_{i=1}^n E_{z_i \\sim q_{\\phi}}[\\frac{-||x_i - f_{\\theta}(h_{\\phi}(x_i))||_2^2}{2\\sigma_d^2} \\ log(\\frac{1}{\\sqrt{2\\pi\\sigma_d^2}})] - KL(q_{\\phi}(z_i|x_i) || p_{\\theta}(z_i))$$\n",
    "\n",
    "Therefore, both of AE and VAE are minimizing the squared loss error. The error is called reconstruction loss because it is trying to penaltilize the difference between original input and reconstructed input. But in VAE's cost function we can see KL divergence and that is what makes the difference. This term acts like a regularization term in a sense that the model can not generate latent variables from arbitarary distribution(in AE one extreme scenario is that the AE maps each data point in dataset to a number and then decode them in reverse direction). The regularization will encourage the learned latent variables to have similar distributions to the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4afd3-f9a8-431b-85d0-9c73a8c8e2cc",
   "metadata": {},
   "source": [
    "<a name='3.1.6'></a>\n",
    "#### Hand-written Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c33fb-5281-4328-9037-dd872343e956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
