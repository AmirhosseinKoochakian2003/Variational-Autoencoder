{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b8be54-9eea-4900-8d39-4d9545cbaef2",
   "metadata": {},
   "source": [
    "# Variational autoencoders\n",
    "\n",
    "In this project, we will explore the idea of variational autoencoders. Also, we will implement a variational autoencoder using Tensorflow to generate images like MNIST hand-written digits.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Introduction](#1)\n",
    "- [2 - Autoencoders](#2)\n",
    "    - [2.1 - Dimension Reduction using Autoencoders](#2.1)\n",
    "- [3 - Variational autoencoders](#3)\n",
    "    - [3.1 - Theory behind VAEs](#3.1)\n",
    "        - [3.1.1 - Evidence Lower BOund](#3.1.1)\n",
    "        - [3.1.2 - Variational Inference](#3.1.2)\n",
    "        - [3.1.3 - Reparameterization Trick](#3.1.3)\n",
    "        - \n",
    "        - [3.1.5 - Hand-written Image Generator](#3.1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47500df-22a0-46de-9ff1-5378782bcb7d",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Introduction\n",
    "Variational autoencoders are members of deep generative models that encode the inputs to a lower dimensional space, sample randomly from the latent or encoded space, and decode that sample to generate a new data. More percisely, variational autoencoders can be considered in two different ways:\n",
    "1. VAEs are type of autoencoders. However, there is a key difference between AEs and VAEs that will be discussed when autoencoders are explained.\n",
    "2. VAEs are probabilistic generative models that work with joint distribution of observed and latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ffa3d-ba6c-4eff-b2da-d8245aaa129a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cca20-c2e9-40b7-907a-ce993b22b47a",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### Dimension Reduction using Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca474c4-4ae8-424f-8ce5-b397602af844",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## Variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014ece3-03c2-4c6f-bbf2-1f4c19f769d3",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### Theory behind VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a4cc-ee4b-4170-aa13-81907d806299",
   "metadata": {},
   "source": [
    "<a name='3.1.1'></a>\n",
    "#### Evidence Lower BOund\n",
    "Evidence lower bound (ELBO) is a key element in order to approximate posterior distribution. It is a lower bound on the log marginal likelihood of the data given the model parameters. By maximizing the ELBO, we are effectively minimizing the Kullback-Leibler (KL) divergence between the true posterior distribution and the approximating distribution(i will show it in this section).\n",
    "\n",
    "Before finding this lower bound, it is a good idea to define our problem(i'll define our problem and our notation for each part). Consider we have two random variables, $X$(observation or real data) and $Z$(hidden or latent variable). $X$ and $Z$ are distributed according to a joint distribution $P(X,Z;\\theta)$ where $\\theta$ parameterizes the distribution.\n",
    "\n",
    "Now we need to determine what is the evidence. The evidence is simply a log-likelihood of observations $x$ with fixed $\\theta$. Intuitively, likelihood function shows how much our model and parameter($\\theta$) align with observations. High value of likelihood function indicates that the model is appropriate for the given data. So, now the goal is to find a lower bound for $p(x;\\theta)$. Assume that $Z$ follows $q$ distribution. Now we can use marginalization to achieve that lower bound as follows:\n",
    "\n",
    "$$\\begin{align} log p(x;\\theta) &= log\\int_{z} p(x,z;\\theta) dz \\\\ &= log\\int_{z} p(x,z;\\theta) \\frac{q(z)}{q(z)} dz \\\\ &= log E_{z\\sim q}[\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ & \\geq^* E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}]\\end{align}$$\n",
    "\n",
    "$$\\begin{align} => ELBO = E_{z\\sim q}[log \\frac{p(x,Z;\\theta)}{q(Z)}] \\ (1) \\end{align}$$\n",
    "\n",
    "\\* : This inequality is the result of [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality). Since $log$ is a concave function, $log(E[X]) \\geq E[log(X)]$ where X in this statement is $\\frac{p(x,Z;\\theta)}{q(Z)}$.\n",
    "\n",
    "Now lets prove why maximizing the ELBO leads to minimizing the KL divergence between the true posterior distribution and the approximating distribution. Assume that we want to find a $q$ distribution that is the most accurate distribution in order to approximate $p(z|x;\\theta)$.(in VAEs we need to find an approximation for $p(z|x;\\theta)$, so it is needed to find a close distribution for that and it is the reason why i explain the statement with this distribution).\n",
    "\n",
    "$$\\begin{align} KL \\ (q(z) \\ || \\ p(z|x;\\theta)) &= E_{Z \\sim q}[log \\frac{q(Z)}{p(Z|x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z|x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\frac {p(Z,x;\\theta)}{p(x;\\theta)}] \\\\ &= E_{Z \\sim q}[log \\ q(Z)] - E_{Z \\sim q}[log \\ p(Z,x;\\theta)] + E_{Z \\sim q}[log \\ p(x;\\theta)] \\\\ &= E_{Z \\sim q}[log \\ p(x;\\theta)] - E_{Z \\sim q}[log\\frac{p(x,Z;\\theta)}{q(Z)}] \\\\ &=^{*,(1)} log \\ p(x;\\theta) - ELBO \\\\ &= evidence - ELBO\\end{align}$$\n",
    "\n",
    "\\* : Notice that $log \\ p(x;\\theta)$ is not dependent on $Z$, so it acts like a constant and comes out of the expectation.\n",
    "\n",
    "Now, notice that when $\\theta$ is fixed and we are looking for a q that minimize the KL divergence, evidence acts as constant, so becuase of negative sign of ELBO, maximizing the ELBO leads to minimizing the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff2f0a-dde6-44d0-a97b-16a728f15c53",
   "metadata": {},
   "source": [
    "<a name='3.1.2'></a>\n",
    "#### Variational Inference\n",
    "\n",
    "In many practical scenarios, computing the exact posterior distribution is intractable due to the complexity of the model or the size of the data. Variational inference provides a framework to approximate this posterior distribution with a simpler distribution chosen from a parameterized family of distributions. \n",
    "The essence of variational inference is to pose the problem of approximating the posterior distribution as an optimization problem. The goal is to find the member of the chosen family of distributions that minimizes the Kullback-Leibler (KL) divergence from the true posterior distribution.\n",
    "By maximizing the Evidence Lower Bound (ELBO), which is a lower bound on the log marginal likelihood of the data given the model parameters, variational inference seeks to find the best approximation to the true posterior distribution given the model and the observed data.\n",
    "\n",
    "When we have a model with both hidden (Z) and observed (X) variables, and we want to figure out the likelihood of Z given X, variational inference helps ous to find a reasonable approximation. But, first lets check why we can not calculate $p(z|x)$ explicitly. Using Bayes theorem, $p(z|x)$ can be written as follows:\n",
    "$$\\begin{align} p(z|x) = \\frac{p(z,x)}{p(x)} = \\frac{p(x|z) \\times p(z)}{\\int_z p(z,x) dz} \\end{align}$$\n",
    "\n",
    "The important point is that calculating the denominator is not always feasible. Therefore, variational inference is used.\n",
    "\n",
    "In this technique, we consider a family of distributions named variational distribution family and aim to find a $q$ that $q(z)$ be the closest distribution to $p(z|x)$ in the variational distribution family. Also, assume that $\\phi$ controls these distributions.(it is the variational parameter). So, our goal is to find a $\\phi$ that minimize the KL divergence between $q_{\\phi}(z)$ and $p(z|x)$. Interestingly, based on what we showed in [Evidence Lower BOund](#3.1.1), instead of minimizing the KL divergence, we can maximize the ELBO to find the best approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5996d0c-b686-480d-9cbf-f5d271642e53",
   "metadata": {},
   "source": [
    "<a name='3.1.3'></a>\n",
    "#### Reparameterization Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91922b26-cda0-407b-a483-a90bd56b5c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
